io:
  logdir: logs/
  resume_from:
  num_workers: 4
  tensorboard_port: 0
  validation_interval: 2  # validate every N epochs
  run_name: gfrid_roofmapnet_rid2_v2

model:
  data_sources: [gfrid, roofmapnet, rid2]
  # Per-dataset sampling weights for balanced training.
  # "uniform" = flat concatenation (default), "balanced" = equal per-dataset,
  # "sqrt" = sqrt-proportional (moderate upweighting of small datasets),
  # or a list of explicit weights matching data_sources order, e.g. [3.0, 1.0, 1.0]
  sampling_strategy: [3.0, 1.0, 1.0]  # boost gfrid, keep roofmapnet/rid2 proportional
  image:
      mean: [117.490, 119.090, 114.480]
      stddev: [56.780, 52.650, 53.820]

  batch_size: 8
  batch_size_eval: 2

  # backbone multi-task parameters
  head_size: [[2], [1], [2]]
  loss_weight:
    jmap: 4.0
    lmap: 0.5
    joff: 0.25
    lpos: 2
    lneg: 1.5

  # backbone parameters
  backbone: stacked_hourglass
  depth: 4
  num_stacks: 2
  num_blocks: 1

  # sampler parameters
  ## static sampler
  n_stc_posl: 300
  n_stc_negl: 80

  ## dynamic sampler
  n_dyn_junc: 300
  n_dyn_posl: 300
  n_dyn_negl: 80
  n_dyn_othr: 600

  # LOIPool layer parameters
  n_pts0: 32
  n_pts1: 8

  # line verification network parameters
  dim_loi: 128
  dim_fc: 1024

  # maximum junction and line outputs
  n_out_junc: 150
  n_out_line: 1000

  # additional ablation study parameters
  use_cood: 0
  use_slop: 0
  use_conv: 0

  # junction threashold for evaluation (See #5)
  eval_junc_thres: 0.008

  augmentation:
    flip_prob: 0.5
    rotate_prob: 0
    rotate_max_angle: 90

optim:
  name: Adam
  lr: 1.0e-4
  amsgrad: True
  weight_decay: 1.0e-4
  max_epoch: 120
  lr_decay_epoch: 10

  # LR scheduler: "cosine_warmup", "plateau", or "step" (original behavior)
  #   cosine_warmup -- linear warmup then cosine decay (needs max_epoch tuned)
  #   plateau       -- ReduceLROnPlateau, adapts to val loss (good for exploration)
  #   step          -- single step decay at lr_decay_epoch
  scheduler: plateau
  # warmup_epochs: 5
  # min_lr: 1.0e-6
  # plateau-specific (used only when scheduler: plateau)
  plateau_factor: 0.5
  plateau_patience: 5

  # Early stopping: stop if val loss hasn't improved for this many epochs.
  # Set to 0 to disable.
  early_stopping_patience: 15
